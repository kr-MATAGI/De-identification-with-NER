## 논문 1 - BERT를 활용한 한국어 개체명 인식기
  \- 저자: 황석현, 신석환, 최동근, 김성현, 김재은 (솔트록스, 솔트록스 파트너스)  
  \- 논문 링크: [BERT를 활용한 한국어 개체명 인식기](https://papersearch.net/thesis/article.asp?key=3861445)  
  
  ### 요약
  \- 15개의 개체명 태그(TTA 표준)으로 분류  
  \- 본 논문에서는 구글에서 공개한 BERT-multilingual 모델을 기반으로 성능향상을 위해 WIKI 데이터를 활용하여 연장학습을 수행  
  \- 본 논문에서 제안하는 BERT-NER은 BERT 모델에 한 개의 layer를 추가하여 해당 layer에서 개체명을 태깅하도록 함  
  \- 비교 대상은 Bi-LSTM + Attention + CRF 모델을 사용  
  \- 학습 데이터: 95,786문장에 257,387개의 태그가 태깅된 데이터, 평가 데이터: 10,502문장에 27,781개의 개체명 태그 데이터
  
  ### 모델 별 성능
  
  <img width="553" alt="image" src="https://user-images.githubusercontent.com/30927066/156992211-2e67a438-721d-4a4e-9c44-b6625d26a099.png">

  ### 태그 별 성능 (Bi-LSTM + Attention + CRF / BERT-NER)
  
  <img width="493" alt="image" src="https://user-images.githubusercontent.com/30927066/156992363-0ce4a2cb-55e3-41f7-adb6-58920870e1ae.png">

  ### 태그별 성능 (BERT-NER / BERT-NER+)
  
  <img width="503" alt="image" src="https://user-images.githubusercontent.com/30927066/156992536-6895b58f-86f4-406b-9490-14ae91e94e44.png">
