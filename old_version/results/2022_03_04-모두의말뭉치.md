## Config

  \- model name: monologg/koelectra-base-v3-discriminator  
  \- architectures: ElectraForTokenClassification  
  \- attention probs dropout prob: 0.1  
  \- embedding size: 768  
  \- hidden act: gelu  
  \- hidden dropout prob: 0.1  
  \- hidden size: 768  
  \- max position embeddings: 512  
  \- num attention heads: 12  
  \- num hidden layers: 12  
  \- vocab size: 35000  
  
  \- train epoch: 5    
  \- learning rate: 5e-5  
  \- optimizer: AdamW  
  \- train batch size: 8  
  \- eval batch size: 8  
  
## Datasets size
  \- Train : 939,701  
  \- Dev : 134,243  
  \- Test : 268, 487 (Not Used)  
  
## Results
  
  - epoch 5 (Dev)
  
|           | precision | recall  | f1-score  | support |
| :-------: | :-------: | :-----: | :-------: | :-----: |
| AF        |  0.63     | 0.57    | 0.60      | 4595    |
| AM        |  0.81     | 0.83    | 0.82      | 3390    |
| CV        |  0.82     | 0.85    | 0.83      | 13751   |
| DT        |  0.91     | 0.94    | 0.92      | 5941    |
| EV        |  0.56     | 0.57    | 0.57      | 480     |
| FD        |  0.66     | 0.65    | 0.65      | 111     |
| LC        |  0.89     | 0.92    | 0.91      | 7859    |
| MT        |  0.58     | 0.64    | 0.61      | 121     |
| OG        |  0.69     | 0.74    | 0.72      | 5964    |
| PS        |  0.74     | 0.73    | 0.73      | 12650   |
| PT        |  0.70     | 0.71    | 0.71      | 906     |
| QT        |  0.81     | 0.84    | 0.82      | 3830    |
| TI        |  0.86     | 0.88    | 0.87      | 1448    |
| TM        |  0.73     | 0.75    | 0.74      | 2859    |
| TR        |  0.49     | 0.34    | 0.40      | 97      |
|           |           |         |           |         |
| micro avg | 0.79      | 0.80    | 0.79      | 64002   |
| macro avg | 0.73      | 0.73    | 0.73      | 64002   |
| weighted avg | 0.79   | 0.80    | 0.79      | 64002   |


