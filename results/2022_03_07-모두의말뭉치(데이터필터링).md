## Config

  \- model name: monologg/koelectra-base-v3-discriminator  
  \- architectures: ElectraForTokenClassification  
  \- attention probs dropout prob: 0.1  
  \- embedding size: 768  
  \- hidden act: gelu  
  \- hidden dropout prob: 0.1  
  \- hidden size: 768  
  \- max position embeddings: 512  
  \- num attention heads: 12  
  \- num hidden layers: 12  
  \- vocab size: 35000  
  
  \- train epoch: 5    
  \- learning rate: 5e-5  
  \- optimizer: AdamW  
  \- train batch size: 8  
  \- eval batch size: 8  
  
## Datasets size
  \- Train : 389,505  
  \- Dev : 55,643  
  \- Test : 111,289 (Not Used)  
  
## Results
  
  - epoch 5 (Dev)
  
|           | precision | recall  | f1-score  | support |
| :-------: | :-------: | :-----: | :-------: | :-----: |
| AF        |  0.58     | 0.59    | 0.59      | 6252    |
| AM        |  0.84     | 0.88    | 0.86      | 4150    |
| CV        |  0.82     | 0.87    | 0.84      | 15257   |
| DT        |  0.92     | 0.94    | 0.93      | 7247    |
| EV        |  0.56     | 0.55    | 0.56      | 626     |
| FD        |  0.76     | 0.81    | 0.78      | 170     |
| LC        |  0.87     | 0.91    | 0.89      | 7918    |
| MT        |  0.55     | 0.67    | 0.61      | 162     |
| OG        |  0.64     | 0.72    | 0.68      | 6139    |
| PS        |  0.75     | 0.75    | 0.75      | 17592   |
| PT        |  0.67     | 0.71    | 0.69      | 901     |
| QT        |  0.83     | 0.87    | 0.85      | 4868    |
| TI        |  0.90     | 0.92    | 0.91      | 1939    |
| TM        |  0.70     | 0.76    | 0.73      | 3888    |
| TR        |  0.50     | 0.48    | 0.49      | 134     |
|           |           |         |           |         |
| micro avg | 0.78      | 0.81    | 0.79      | 77243   |
| macro avg | 0.73      | 0.76    | 0.74      | 77243   |
| weighted avg | 0.78   | 0.81    | 0.79      | 77243   |

