## Config

  \- model name: monologg/koelectra-base-v3-discriminator  
  \- architectures: ElectraForTokenClassification  
  \- attention probs dropout prob: 0.1  
  \- embedding size: 768  
  \- hidden act: gelu  
  \- hidden dropout prob: 0.1  
  \- hidden size: 768  
  \- max position embeddings: 512  
  \- num attention heads: 12  
  \- num hidden layers: 12  
  \- vocab size: 35000  
  
  \- train epoch: 5  
  \- train batch size: 8  
  \- learning rate: 5e-5  
  \- optimizer: AdamW  
  \- eval batch size: 8  
  
## Results
  
  - epoch 5
  
|     | precision | recall  | f1-score  | support |
| ----| --------- | ------- | --------- | ------- |
| AF  |  0.63     | 0.57    | 0.60      | 4595    |
