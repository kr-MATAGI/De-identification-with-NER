# 각 Datasets / Model 별 성능 비교

## 모두의 말뭉치
  - 

## Naver NER
  1) monologg/KoELECTRA (Base Model)  
    - [HuggingFace_monologg/koelectra](https://huggingface.co/monologg/koelectra-base-v3-discriminator)  
     
      ~~~json
      {
        "max_seq_len": 128,
        "num_train_epoch": 20,
        "weight_decay": 0.0,
        "gradient_accumulation_steps": 1,
        "adam_epsilon": 1e-8,
        "warmup_proportion": 0,
        "max_steps": -1,
        "max_grad_norm": 1.0,
        "model_type": "koelectra-base-v3",
        "seed": 42,
        "train_batch_size": 32,
        "eval_batch_size": 128,
        "learning_rate": 5e-5
      }
      ~~~ 
  - Results  
  ![image](https://user-images.githubusercontent.com/30927066/157807600-c9f9ad48-4609-464c-866f-5a6c33aaeea3.png)

